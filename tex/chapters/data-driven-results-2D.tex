\section{The 2D Shallow Water Equations with Gaussian initial conditions}\label{sec:data-driven-results-2D}
In this section we present the results for the 2D SWE using data-driven models. 
The initial condition, a Gaussian function, is the same as in the 1D case but extended to two dimensions.
We solve the 2D SWE using both a CNN and and FNO model, comparing their performance in terms of run time and accuracy.
Additionally, we compare their run time to the FVM to evaluate whether the data-driven models can serve as a faster alternative.
We also assess the models' ability to transfer solutions across grids, by training the models on a coarse grid and making predictions on a finer grid.
Finally, we evaluate the models' capability to generalize further in time, testing their long-term predictive performance.

The initial condition for the 2D problem is a Gaussian function as given in~\eqref{eq:2D_swe_ic_gaussian}.
The initial condition is illustrated in \autoref{fig:2D_gauss_initial_condition}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_gauss_initial_condition.pdf}
    \caption{Initial condition for the 2D SWE.}\label{fig:2D_gauss_initial_condition}
\end{figure}
In \autoref{fig:2D_gauss_initial_condition} we see the initial condition for the 2D SWE problem, which is close to the 2D dam break problem, only here with a Gaussian function.
The solution is generated from $t = 0$ to $t = 5$ s.
Some information of the data used for the 2D SWE generated by the FVM can be found in \autoref{tab:data_2D_SWE}.
\begin{table}[H]
    \centering
    \begin{tabular}{c|ccccc}
        \textbf{Case} & \textbf{n\_train} & \textbf{n\_val} & \textbf{n\_test} & $\mathbf{\Delta x}$ & $\mathbf{\Delta t}$ \\
        \hline
        2D SWE, N = 64 & 48 & 16 & 17 & 0.625 m  & $[0.050 \text{ s}, 0.073 \text{ s}]$ \\
        2D SWE, N = 128 & 99 & 33 & 33 & 0.3125 m  & $[0.025 \text{ s}, 0.035 \text{ s}]$ \\
    \end{tabular}
    \caption{Details of the used data for the case with the 2D SWE for both a number of grid points $N=64$ and $N=128$.}\label{tab:data_2D_SWE}
\end{table}
In \autoref{tab:data_2D_SWE} we see that the train/validation/test data split is $60\%/20\%/20\%$ for both $N = 64$ and $N = 128$.
In the FVM we use a variable time step size $\Delta t$ to ensure stability, which is why the time step size varies.
We see that for $N = 64$ the time step size varies between $0.050$ s and $0.073$ s, while for $N = 128$ the time step size varies between $0.025$ s and $0.035$ s.
The time step size for the higher $N$ is in general smaller than for the lower $N$ to ensure stability.
This is also what gives more data points for $N = 128$ compared to $N = 64$.


\subsubsection*{CNN Model}
We train a CNN model with several convolutional layers and ReLU activation functions.
We use the Adam optimizer with a learning rate of $0.001$ and a batch size of $32$.
We make predictions from $t = 0$ to $t = 5$.
The model is trained for $500$ epochs.
The training and validation loss for the 2D CNN model can be seen in \autoref{fig:2D_CNN_loss}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_CNN_loss.pdf}
    \caption{Training and validation loss for the 2D CNN model.}\label{fig:2D_CNN_loss}
\end{figure}
\autoref{fig:2D_CNN_loss} shows that the training and validation loss are decreasing, as a function of the number of epochs.
The error plot for the last prediction for the 2D CNN can be seen in \autoref{fig:2D_CNN_error}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_CNN_error.pdf}
    \caption{Error plot for the last prediction for the 2D CNN.}\label{fig:2D_CNN_error}
\end{figure}
From \autoref{fig:2D_CNN_error}, we see that overall the absolute error is small.
The error is largest at the boundaries of the domain. 
This may be due to the fact that in this case we are working with boundary conditions simulating a wall.
Hence, when the wave hits the wall, the wave is reflected, and the model has to learn the reflection of the wave.
This may also lead to discontinuities in the solution, which can be difficult for the model to handle.

\subsubsection*{FNO Model}
We train a FNO model with the same training/validation/testing split as for the CNN model.
The FNO model uses 8 Fourier modes, the Adam optimizer with a learning rate of $0.001$, and a batch size of $10$.
The model is trained for $500$ epochs.
The training and validation loss for the 2D FNO model can be seen in \autoref{fig:2D_FNO_loss}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_FNO_loss.pdf}
    \caption{Training and validation loss for the 2D FNO model.}\label{fig:2D_FNO_loss}
\end{figure}
From \autoref{fig:2D_FNO_loss} we see that the training and validation loss are decreasing, and more or less stable with some fluctations. 
The plot suggests that the model has converged.
The error plot for the last prediction for the 2D FNO can be seen in \autoref{fig:2D_FNO_error}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_FNO_error.pdf}
    \caption{Error plot for the last prediction for the 2D FNO.}\label{fig:2D_FNO_error}
\end{figure}
As for the CNN model, the error is largest at the boundaries.
However, we notice that the absolute error is smaller for the FNO model compared to the CNN model.

\subsubsection*{Comparison}
We compare the performance of the CNN and FNO models in terms of the MSE and MAE for the predictions.
We test for both $N = 64$ and $N = 128$ grid points in each direction.
The errors are calculated for the test data, i.e., the last $20\%$ of the time steps.
The results are summarized in \autoref{tab:results_2D_comparison}, together with the time for training the models.
\begin{table}[H]
    \centering
    \small % Reduce font size
    \begin{tabular}{c|cccc|cccc}
        Model & \multicolumn{4}{c|}{$N = 64$} & \multicolumn{4}{c}{$N = 128$} \\
        \cline{2-9}
        & Epochs & MSE & MAE & Time (s) & Epochs & MSE & MAE & Time (s) \\
        \hline
        CNN  &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=64_nepochs.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=64_MSE_test.txt} & 
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=64_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=64_time.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=128_nepochs.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=128_MSE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=128_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_Nx=128_time.txt} 
        \\
        \hline
        FNO  &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=64_nepochs.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=64_MSE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=64_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=64_time.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=128_nepochs.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=128_MSE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=128_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_Nx=128_time.txt}
        \\
        \hline
    \end{tabular}
    \caption{Test loss in terms of MSE and MAE, and time for training the models for the 2D SWE.}\label{tab:results_2D_comparison}
\end{table}
The errors are low and similar for both models.
However, we also observe that the FNO model generally requires more time to train.
Based on theory, FNOs have demonstrated potential in transferring solutions across grids.
To test this, we train the models on a coarse grid and then make predictions on a finer grid.
The table below presents the results when the models are trained on a grid with $N = 64$ and subsequently used to make predictions on finer grids with $N = 128$ and $N = 256$.
\begin{table}[H]
    \centering
    \begin{tabular}{c|ccc|ccc}
        Model & \multicolumn{3}{c}{$N = 128$} & \multicolumn{3}{c}{$N = 256$} \\
        \cline{2-7}
        & MSE & MAE & Prediction time [s] & MSE & MAE & Prediction time [s] \\
        \hline
        CNN &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_train_Nx=64_test_N=128_MSE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_train_Nx=64_test_N=128_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_train_Nx=64_test_N=128_time.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_train_Nx=64_test_N=256_MSE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_train_Nx=64_test_N=256_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_CNN_train_Nx=64_test_N=256_time.txt} \\ \hline
        FNO  &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_train_Nx=64_test_N=128_MSE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_train_Nx=64_test_N=128_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_train_Nx=64_test_N=128_time.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_train_Nx=64_test_N=256_MSE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_train_Nx=64_test_N=256_MAE_test.txt} &
        \input{C:/Users/Matteo/Shallow-Water-Equations/saved_results/2D_FNO_train_Nx=64_test_N=256_time.txt} \\
    \end{tabular}
    \caption{Test loss in terms of MSE and MAE, and time for training the FNO model on a grid with $N = 64$ and then making predictions on a grid with $N = 128$ and $N=256$.}\label{tab:results_2D_train_64_test_128_256}
\end{table}
From the results in \autoref{tab:results_2D_train_64_test_128_256} we see that FNO model achieves small errors both for $N = 128$ and $N = 256$, indicating its ability to generalize to a finer grid.
The CNN model shows higher errors, but might still be accurate enough depending on the application.
This ability to train the FNO model (and partially, also the CNN model) on a coarse grid and make predictions on a finer grid is a significant advantage.
In htis study, it is particularly useful for solving the SWE numerically, as solving the SWE on a fine grid using the FVM is computationally expensive.

By comparing to the run time of the FVM, shown in \autoref{tab:scalability}, we observe that for $N = 64$, the FVM is faster than the data-driven models.
For $N = 128$, the CNN is faster than the FVM, while the FNO is slower.
This holds both when the models are trained on a grid with $N = 128$ and when they are trained on a grid with $N = 64$ and then used to make predictions on a grid with $N = 128$.
For $N = 256$, both models are fasyer than the FVM when trained on a grid with $N = 64$ and then using grid transferability, maintaining the same accuracy as for $N = 64$.
This way, the CNN model is over 60 times faster than the FVM, while the FNO model is over 7 times faster.
This comparison includes the time for training the models.
However, it is important to note that once trained, the models can generate predictions much faster than the FVM, even for new initial conditions.
The results suggest that the potential for using data-driven methods to solve the SWE is particularly significant for larger grid sizes, where the FVM becomes computationally expensive.
This represents a major advantage, as it significantly reduces computational time and energy consumption, addressing the scalability challenges faced when solving the SWE numerically.


\subsection{Long-term predictions for the 2D Shallow Water Equations}
In this section, we evaluate the models' ability to generalize to longer time horizons.
The previous models were trained on data generated using the FVM with vairable time step sizes.
To control the specific time points for long-term predictions, a fixed time step size is required.
Consequenlty, we have generated data for the 2D SWE using $N = 64$ points in each spatial direction and a fixed time step size of $\Delta t = 0.025$ s.
To facilitate testing long-term predictions, the data set spans a longer time interval, covering $t = 0$ s to $t = 15$ s.
The models are trained on the data from $t = 0$ s to $t = 8$ s, validated on the data from $t = 8$ s to $t = 10$ s, and tested for long-term prediction capability using data from $t = 10$ s to $t = 15$ s.
Information of the data used for the 2D SWE with a fixed time step size can be found in \autoref{tab:data_2D_SWE_fixed_time_step}.
\begin{table}[H]
    \centering
    \begin{tabular}{c|ccccc}
        \textbf{Case} & \textbf{n\_train} & \textbf{n\_val} & \textbf{n\_predictions} & $\mathbf{\Delta x}$ & $\mathbf{\Delta t}$ \\
        \hline
        2D SWE & 480 & 120 & 200 & 0.625 m  & $0.025$ \\
    \end{tabular}
    \caption{Details of the used data for the case with the 2D SWE with a fixed time step size.}\label{tab:data_2D_SWE_fixed_time_step}
\end{table}
We generate predictions for up to $n = 200$ time steps into the future, corresponding to the interval from $t = 10$ s to $t = 15$ s.
The validation data points are crucial for selecting the best model for long-term predictions and prevent overfitting.
Initially, we adopted an iterative approach, where the model made a prediction for the next time step and used that prediction as input for the next time step.
While this approach produces acceptable results for the inital time steps, the error increases as we make predictions further in time, probably due to accumulating errors.
To address this issue, we implemented an alternative approach by creating sequences of data and training the model to predict the next time step based on the entire sequence.
In this method, the most recent prediction becomes part of a sequence, distributing the influence across subsequent predictions, rather than heavily impacting the next prediction alone.
This approach aims to stabilize long-term predictions.

\subsubsection*{CNN Model}
We train the CNN model on the described data set for $100$ epochs.
The model consists of several convolutional layers with ReLU activation functions.
The sequence length is set to $50$ and the model is trained using the Adam optimizer with a learning rate of $0.001$ and a batch size of $16$.
We make predictions for up to $n = 200$ time steps into the future, and the error plot for the prediction after $n = 20$ time steps, corresponding to $0.5$ s, can be seen in \autoref{fig:2D_CNN_long_term_error}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_CNN_long_term_predictions_error_Ntrain=64_Npred=64_idx=20.pdf}
    \caption{Error plot for the long-term prediction for the 2D CNN model.}\label{fig:2D_CNN_long_term_error}
\end{figure}
In \autoref{fig:2D_CNN_long_term_error} we see that the error is more equally distributed over the domain compared to the short-term predictions in \autoref{fig:2D_CNN_error}.
The prediction, together with the numerical solution can be seen in \autoref{fig:2D_CNN_long_term_predictions} in Appendix \autoref{app:2D_SWE_long_term_prediction}.


\subsubsection*{FNO Model}
We run the same test for the FNO model, training it on the same data set as the CNN model, and also for $100$ epochs.
The model has 12 Fourier modes and is trained using the Adam optimizer with a learning rate of $0.001$ and a batch size of $16$.
The sequence length is set to $50$.
The error plots of the long-term predictions from the FNO model after 20 time steps, equivalent to $0.5$ s, can be seen in \autoref{fig:2D_FNO_long_term_error}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_FNO_long_term_predictions_error_Ntrain=64_Npred=64_idx=20.pdf}
    \caption{Error plot for the long-term prediction for the 2D FNO model.}\label{fig:2D_FNO_long_term_error}
\end{figure}
From the error plot in \autoref{fig:2D_FNO_long_term_error}, we see that the error is bigger than for the short-term predictions in \autoref{fig:2D_FNO_error}, but for many applications still acceptable.
We also see that error for the FNO model is smaller than for the CNN model, indicating that the FNO model is better able to generalize further in time and make long-term predictions.
A plot of the predictions and the ground truth can be found in \autoref{fig:2D_FNO_long_term_predictions} in Appendix \autoref{app:2D_SWE_long_term_prediction}, where we see that the FNO model is able to capture the dynamics of the solution well.

\subsubsection*{Comparison}
To assess the models performance over time, we plot the error at each time step for long-term predictions up to $n = 200$ time steps into the future.
Then CNN and FNO models are compared in terms of MSE, MAE and the absolute max error for long-term predictions, as shown in \autoref{fig:2D_CNN_FNO_mse_mae_max_error}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{C:/Users/Matteo/Shallow-Water-Equations/plots/2D_CNN_FNO_mse_mae_max_error.pdf}
    \caption{MSE, MAE and max error for the long-term predictions for the 2D CNN and FNO models.}\label{fig:2D_CNN_FNO_mse_mae_max_error}
\end{figure}
When making predictions, it is essential to consider the physical properties of the system, rather than relying solely on the loss functions.
The choice of error metric should align with the goals of the predictions.
For instance, it may be more important to minimize the MSE or the maximum absolute error, depending on the physical properties of the system.
This raises a key question: is it better to have one large error or many small errors?

As shown in \autoref{fig:2D_CNN_FNO_mse_mae_max_error}, the error increases as predictions are made further in time.
Overall, the FNO model exhibits lower errors compared to the CNN model, indicating it may offer more accurate long-term predictions.
We have plotted the predictions made with the CNN and FNO models for $t = 12.5$ s in \autoref{fig:2D_CNN_long_term_predictions_2} and \autoref{fig:2D_FNO_long_term_predictions_2}.
These figures can be found in Appendix \autoref{app:2D_SWE_long_term_prediction}.
Notably, the FNO model demonstrates the ability to generalize further in time, maintaining relatively low errors even after $n = 200$ time steps, comparable to the errors observed during the initial time steps.
This is particularly impressive, as small errors early in forecasting applications often lead to significant errors over time.
The FNO model appears capable of stabilizing errors and providing accurate predictions over long time horizons, highlighting its potential for long-term predictions.


\section{Summary}
In this section, we will shortly describe our key finding for the results for both numerical and data-driven methods.

\begin{itemize}
    \item \textbf{1D SWE}: CNN outperforms FNO in terms of accuracy and run time. 
        When presented for a new initial condition, FNO maintained accuracy, while CNO showed a slight drop in precision.
        The run time for the FVM is not analysed, as it is very fast (a few seconds).
        No scalability issues were encountered in the 1D domain
    \item \textbf{1D LSWE on a sphere}: CNN outperforms FNO in both accuracy and run time.
    For the steepest initial condition, the MSE and MAE of CNN and FNO are almost identical.
    The FVM run time is not analysed, as it is significantly faster than the data-driven models.
    \item \textbf{2D SWE}: In general, the FNO demonstrated higher accuracy, while the CNN was faster when trained and making predictions on grids with $N = 64$ and $N = 128$.
        \begin{itemize}
            \item \textbf{Grid transferability}: Both models maintained accuracy when trained on coarse grids and used for predictions on finer grids.
            However, FNO outperformed CNN in terms of MSE and MAE.
            \item \textbf{Run time}: For $N \leq 64$, FVM is faster than both models.
            At $N = 128$, CNN is faster than FVM, while FNO is slower.
            At $N = 256$, using grid transferability, both CNN and FNO are faster than FVM.
            The CNN model is over 60 times faster than the FVM, while the FNO model is over 7 times faster (including training time).
            \item \textbf{Long-term predictions}: Both models exhibit low error in the initial time steps.
            Over longer time horizons, CNN's error increases, whereas FNO maintains accuracy, highlighting its potential for long-term predictions.
            \end{itemize}
\end{itemize}
This concludes the results for the numerical and data-driven methods.
In the next section, we will discuss our findings, focusing on the advantages and limitations of the FVM and data-driven approaches, and their potential applications in solving the SWE.
We will also explore potential directions for future research, building on the results presented in this thesis.

